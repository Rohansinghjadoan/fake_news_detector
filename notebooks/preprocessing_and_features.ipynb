{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../data/liar_dataset.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rohan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/rohan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/rohan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Says the Annies List political group supports ...\n",
       "1        When did the decline of coal start? It started...\n",
       "2        Hillary Clinton agrees with John McCain \"by vo...\n",
       "3        Health care reform legislation is likely to ma...\n",
       "4        The economic turnaround started at the end of ...\n",
       "                               ...                        \n",
       "12786    Says his budget provides the highest state fun...\n",
       "12787                      Ive been here almost every day.\n",
       "12788    In the early 1980s, Sen. Edward Kennedy secret...\n",
       "12789    Says an EPA permit languished under Strickland...\n",
       "12790    Says the governor is going around the state ta...\n",
       "Name: Statement, Length: 12791, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Statement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to dataset\n",
    "df['Clean_Statement'] = df['Statement'].apply(clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract lexical + POS features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos(text, tag_prefix):\n",
    "    return len([word for word, tag in pos_tag(word_tokenize(text)) if tag.startswith(tag_prefix)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df['word_count'] = df['Clean_Statement'].apply(lambda x: len(x.split()))\n",
    "df['avg_word_len'] = df['Clean_Statement'].apply(lambda x: sum(len(w) for w in x.split()) / (len(x.split())+1e-5))\n",
    "df['adj_count'] = df['Clean_Statement'].apply(lambda x: count_pos(x, 'JJ'))  # adjectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF feature extraction (unigram + bigram):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "X_tfidf = tfidf.fit_transform(df['Clean_Statement'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "joblib.dump(tfidf, '../models/tfidf_vectorizer.pkl')\n",
    "df.to_csv('../data/processed_liar_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
